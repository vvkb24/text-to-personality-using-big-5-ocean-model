# Personality Detection System using Big Five (OCEAN) Model

A research-grade, end-to-end personality trait detection system that analyzes English text to predict Big Five personality traits with continuous scores, percentiles, categorical labels, and evidence-based explanations.

**Version:** 1.1.0 (Production Hardened)  
**Last Updated:** January 6, 2026

## ğŸ¯ Overview

This system implements a state-of-the-art approach to personality detection combining:
- **Transformer-based ML Baseline**: Sentence embeddings + supervised regression
- **LLM-based Inference**: Google Gemini for contextual analysis and evidence extraction
- **Ensemble Learning**: Weighted combination with learned weights and calibration

### Key Features

- âœ… **Continuous OCEAN Scores** (0-1 scale)
- âœ… **Percentile Rankings** (clamped to 1-99 for safety)
- âœ… **Category Labels** (Low / Medium / High)
- âœ… **Evidence Sentences** for each trait prediction
- âœ… **Confidence Scores** based on text length and model agreement
- âœ… **Production-Safe API** with comprehensive error handling
- âœ… **Web Interface** with React frontend

## ğŸ†• What's New in v1.1.0

### Production Hardening
- **Percentile Safety**: All percentiles clamped to [1.0, 99.0] to avoid extreme values
- **Confidence Estimation**: Per-trait confidence scores based on text length and ML/LLM agreement
- **Text Validation**: Graceful error handling with user-friendly messages
- **Backend Singleton**: ML model loaded once at startup for fast response times
- **Normalized Endpoints**: Both `/predict` and `/predict/` work identically
- **Frontend Safety**: Guards against missing fields and malformed responses

See [PRODUCTION_HARDENING_CHANGELOG.md](PRODUCTION_HARDENING_CHANGELOG.md) for full details.

## ğŸ“Š The Big Five (OCEAN) Model

| Trait | Description | High Scorers | Low Scorers |
|-------|-------------|--------------|-------------|
| **O**penness | Creativity, curiosity, intellectual interests | Creative, curious, imaginative | Conventional, practical |
| **C**onscientiousness | Organization, dependability, self-discipline | Organized, reliable, goal-oriented | Spontaneous, flexible |
| **E**xtraversion | Sociability, assertiveness, positive emotions | Outgoing, energetic, talkative | Reserved, quiet, solitary |
| **A**greeableness | Cooperation, trust, empathy | Helpful, trusting, empathetic | Competitive, skeptical |
| **N**euroticism | Emotional instability, anxiety | Anxious, moody, stressed | Calm, stable, resilient |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Input Text                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                               â”‚
          â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML Baseline       â”‚       â”‚   LLM Inference     â”‚
â”‚   - SBERT Embeddingsâ”‚       â”‚   - Gemini API      â”‚
â”‚   - Ridge Regressionâ”‚       â”‚   - Evidence Extractâ”‚
â”‚   - Per-trait Modelsâ”‚       â”‚   - Structured JSON â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                             â”‚
          â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
          â””â”€â”€â”€â”€â–ºâ”‚   Ensemble      â”‚â—„â”€â”€â”€â”€â”˜
                â”‚   - Learned     â”‚
                â”‚     Weights     â”‚
                â”‚   - Calibration â”‚
                â”‚   - Percentiles â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: Scores, Percentiles, Categories, Evidence          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
personality-detection/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.yaml        # Configuration file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py       # Data loading and preprocessing
â”‚   â”œâ”€â”€ ml_baseline.py       # ML baseline model
â”‚   â”œâ”€â”€ llm_inference.py     # LLM inference engine
â”‚   â”œâ”€â”€ ensemble.py          # Ensemble and calibration
â”‚   â”œâ”€â”€ evaluation.py        # Evaluation framework
â”‚   â”œâ”€â”€ ablation.py          # Ablation studies
â”‚   â”œâ”€â”€ pipeline.py          # Unified inference pipeline
â”‚   â”œâ”€â”€ production_utils.py  # NEW: Production safety utilities
â”‚   â””â”€â”€ utils.py             # Helper utilities
â”œâ”€â”€ web_backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI backend (hardened)
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ web_frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ components/      # React components (with safety guards)
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ train.py                 # Main training script
â”œâ”€â”€ demo.py                  # Quick demo
â”œâ”€â”€ analyze_essay.py         # Custom text analysis
â”œâ”€â”€ example_inference.py     # Inference examples
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ .env                     # Environment variables
â”œâ”€â”€ PRODUCTION_HARDENING_CHANGELOG.md  # v1.1.0 changes
â””â”€â”€ README.md               # This file
```

## ğŸš€ Quick Start

### Installation

```bash
# Clone or navigate to the project
cd "personality detection vscode"

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### Configuration

1. Set up your Gemini API key in `.env`:
```
GEMINI_API_KEY=your_api_key_here
```

2. Or use the mock LLM for testing without an API key.

### Training

```bash
# Basic training with synthetic data
python train.py --data-source synthetic --min-samples 1000

# Training with HuggingFace dataset
python train.py --data-source huggingface --dataset-name "Fatima0923/Automated-Personality-Prediction"

# Training with LLM integration
python train.py --use-llm --llm-samples 100

# Full training with ablation studies
python train.py --use-llm --run-ablation --learn-weights
```

### Inference

```bash
# Run example inference
python example_inference.py
```

### Programmatic Usage

```python
from src.pipeline import create_predictor

# Create predictor
predictor = create_predictor(
    api_key="your_gemini_api_key",
    use_mock_llm=False,  # Set True for testing without API
    ml_weight=0.6,
    llm_weight=0.4
)

# Train (or load pre-trained model)
predictor.train(train_texts, train_labels)

# Single prediction
prediction = predictor.predict("Your text here...")
print(prediction.summary())

# Detailed analysis
analysis = predictor.analyze("Your text here...")
print(analysis)

# Batch prediction
predictions = predictor.predict_batch(list_of_texts)
```

## ğŸŒ Web Application

### Start Backend (Port 8080)
```bash
python -m uvicorn web_backend.main:app --host 127.0.0.1 --port 8080
```

### Start Frontend (Port 3000)
```bash
cd web_frontend
npm install  # First time only
npm run dev
```

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check with model status |
| `/predict` | POST | Personality prediction |
| `/predict/` | POST | Same as above (normalized) |
| `/docs` | GET | Swagger UI documentation |

### API Response Schema (v1.1.0)
```json
{
  "scores": {"openness": 0.723, ...},
  "percentiles": {"openness": 78.5, ...},
  "categories": {"openness": "High", ...},
  "evidence": {"openness": ["sentence1", ...], ...},
  "confidences": {"openness": 0.85, ...},
  "traits": {...},
  "text_length": 391,
  "warning": null,
  "model_info": {...}
}
```

## ğŸ“ˆ Methodology

### 1. Data Pipeline

- **Source**: HuggingFace `Fatima0923/Automated-Personality-Prediction` dataset
- **Preprocessing**: URL removal, whitespace normalization, length filtering
- **Split**: 70% train / 15% validation / 15% test
- **Normalization**: Trait scores normalized to [0, 1] range

### 2. ML Baseline

- **Embeddings**: Sentence-BERT (all-MiniLM-L6-v2)
- **Model**: Ridge regression (per-trait)
- **Cross-validation**: 5-fold CV during training

### 3. LLM Inference

- **Model**: Google Gemini 1.5 Flash
- **Prompt Engineering**: Psychologically-grounded prompts with trait definitions
- **Output**: Structured JSON with scores, evidence, and justifications
- **Rate Limiting**: Built-in request throttling

### 4. Ensemble & Calibration

- **Combination**: Weighted average with learnable weights
- **Calibration**: Isotonic regression for score calibration
- **Percentiles**: Based on training distribution
- **Categories**: Threshold-based (33rd/67th percentiles)

## ğŸ“Š Evaluation Metrics

| Metric | Description |
|--------|-------------|
| Pearson r | Correlation between predictions and ground truth |
| Spearman Ï | Rank correlation (robust to outliers) |
| MAE | Mean Absolute Error |
| RÂ² | Coefficient of determination |
| RMSE | Root Mean Square Error |

## ğŸ”¬ Ablation Studies

The system includes comprehensive ablation studies:

1. **Model Comparison**: ML-only vs LLM-only vs Ensemble
2. **Text Length Effect**: Performance across different text lengths
3. **Calibration Impact**: With vs without score calibration
4. **Weight Sensitivity**: Effect of ensemble weight ratios

## ğŸ“‹ Example Output

```
==================================================
PERSONALITY ANALYSIS RESULTS
==================================================

OPENNESS
  Score: 0.723 | Percentile: 78.5 | Category: High
  Evidence:
    - "exploring new ideas and meeting different people..."
    - "researching ancient philosophy..."
  Justification: Text shows curiosity and intellectual interests

CONSCIENTIOUSNESS
  Score: 0.681 | Percentile: 65.2 | Category: Medium
  Evidence:
    - "quite organized with my work..."
    - "always make detailed plans..."
  Justification: Demonstrates planning and organization

EXTRAVERSION
  Score: 0.612 | Percentile: 55.8 | Category: Medium
  Evidence:
    - "enjoy social gatherings..."
  Justification: Moderate social engagement mentioned

AGREEABLENESS
  Score: 0.745 | Percentile: 82.1 | Category: High
  Evidence:
    - "always ready to help when needed..."
    - "easy to talk to..."
  Justification: Shows helpfulness and social consideration

NEUROTICISM
  Score: 0.421 | Percentile: 38.4 | Category: Medium
  Evidence:
    - "Sometimes I feel anxious about upcoming deadlines..."
  Justification: Occasional anxiety but generally stable
```

## âš ï¸ Limitations

1. **Text Dependency**: Predictions are only as good as the text input
2. **Cultural Bias**: Model trained primarily on English, Western data
3. **Context Missing**: Cannot capture situational personality variations
4. **Self-Report Bias**: Training data may reflect self-presentation
5. **Short Text**: Performance degrades with very short inputs (<50 words)

## ğŸ”’ Ethical Considerations

### Responsible Use

- **Consent**: Only analyze text with proper consent
- **Privacy**: Do not store or share analyzed texts
- **Decisions**: Never use for high-stakes decisions (hiring, etc.)
- **Transparency**: Disclose when personality analysis is used

### Bias Awareness

- Training data may not represent all populations
- Results should be interpreted with cultural context
- System should not reinforce stereotypes

### Recommended Applications

âœ… Self-reflection and personal development
âœ… Educational and research purposes
âœ… Content personalization (with consent)
âœ… Team dynamics understanding

### Not Recommended For

âŒ Employment screening
âŒ Clinical diagnosis
âŒ Legal proceedings
âŒ Surveillance

## ğŸ“š References

1. Goldberg, L. R. (1990). An alternative "description of personality": The Big-Five factor structure. *Journal of Personality and Social Psychology*, 59(6), 1216.

2. Pennebaker, J. W., & King, L. A. (1999). Linguistic styles: Language use as an individual difference. *Journal of Personality and Social Psychology*, 77(6), 1296.

3. Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. *EMNLP*.

4. Majumder, N., et al. (2017). Deep learning-based document modeling for personality detection from text. *IEEE Intelligent Systems*, 32(2), 74-79.

## ğŸ“„ License

This project is for educational and research purposes. See LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please read our contributing guidelines and submit pull requests.

## ğŸ“§ Contact

For questions or collaboration, please open an issue on the repository.

---

## ğŸ“ Changelog

### v1.1.0 (January 6, 2026) - Production Hardening
- Added percentile safety (clamped to 1-99)
- Added per-trait confidence scores
- Added text validation with graceful errors
- Backend loads ML model once at startup
- Normalized `/predict` and `/predict/` endpoints
- Frontend guards against missing fields
- New file: `src/production_utils.py`
- New file: `PRODUCTION_HARDENING_CHANGELOG.md`

### v1.0.0 (January 5-6, 2026) - Initial Release
- Core ML pipeline with Sentence-BERT + Ridge regression
- LLM integration with Google Gemini
- Ensemble model with calibration
- FastAPI backend
- React frontend with Tailwind CSS
- Comprehensive documentation

---

**Note**: This is a research implementation. Performance will vary based on the quality and quantity of training data, as well as the specific domain of texts being analyzed.
